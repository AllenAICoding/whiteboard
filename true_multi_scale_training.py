#!/usr/bin/env python3
"""
True Multi-Scale Training with Hierarchical Feature Extraction

è§£æ±ºå•é¡Œï¼šç¢ºä¿ä¸åŒå°ºåº¦çœŸæ­£å­¸ç¿’åˆ°ä¸åŒå±¤æ¬¡çš„èªç¾©ä¿¡æ¯
1. å¾ Transformer ä¸åŒå±¤æå–ç‰¹å¾µï¼ˆè€Œéåƒ…æœ€çµ‚ embeddingï¼‰
2. æ·»åŠ æ­£äº¤æ€§ç´„æŸç¢ºä¿å„å°ºåº¦å­¸ç¿’ä¸åŒç‰¹å¾µ
3. å±¤æ¬¡åŒ–æ³¨æ„åŠ›æ©Ÿåˆ¶å¼•å°ä¸åŒç²’åº¦çš„é—œæ³¨é»
4. å°æ¯”å­¸ç¿’åœ¨ä¸åŒå°ºåº¦é–“å»ºç«‹ä¸€è‡´æ€§ç´„æŸ
"""

import pandas as pd
from datasets import Dataset
from sentence_transformers import SentenceTransformer, losses, evaluation
from sentence_transformers.losses import MultipleNegativesRankingLoss
from torch.utils.data import DataLoader
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
import logging
import os
import time
import numpy as np
from data_prepare import main_generate_train_data_with_hard_mining, create_validation_triplets
from sklearn.preprocessing import LabelEncoder
import matplotlib.pyplot as plt
import matplotlib.ticker as mticker
from datetime import datetime
from sklearn.cluster import DBSCAN, AgglomerativeClustering
from sklearn.metrics import adjusted_mutual_info_score
import json
import re
from transformers import AutoModel, AutoTokenizer

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training.log'),
        logging.StreamHandler()
    ],
    force=True
)
logger = logging.getLogger(__name__)

class MetricsTracker:
    """è¿½è¹¤å’Œè¦–è¦ºåŒ–è¨“ç·´æŒ‡æ¨™ (å¾ train.py å°å…¥)"""

    def __init__(self,
                 save_path='./training_metrics.xlsx',
                 y_lim_ami=(0.3, 1.05),
                 y_lim_loss=None,  # Set to None for dynamic scaling
                 x_tick_interval=None,
                 y_tick_interval_ami=0.05,
                 y_tick_interval_loss=None,  # Set to None for dynamic scaling
                 loss_scale_margin=0.1):
        """åˆå§‹åŒ–æŒ‡æ¨™è¿½è¹¤å™¨"""
        self.save_path = save_path
        self.metrics_data = {
            'epoch': [], 'step': [], 'timestamp': [],
            'AMI_score': [], 'loss': [], 'learning_rate': []
        }
        # åœ–è¡¨å¤–è§€åƒæ•¸
        self.y_lim_ami = y_lim_ami
        self.y_lim_loss = y_lim_loss
        self.x_tick_interval = x_tick_interval
        self.y_tick_interval_ami = y_tick_interval_ami
        self.y_tick_interval_loss = y_tick_interval_loss
        self.loss_scale_margin = loss_scale_margin

        self.fig, self.axes = None, None
        self._initialize_plot()

    def _calculate_dynamic_loss_scale(self, losses):
        """è¨ˆç®—å‹•æ…‹çš„æå¤±åœ–ç¸®æ”¾ç¯„åœ"""
        if not losses or all(x is None or x == 0 for x in losses):
            return (0, 0.1)  # é è¨­ç¯„åœ
        
        # éæ¿¾æ‰ None å’Œ 0 å€¼
        valid_losses = [x for x in losses if x is not None and x != 0]
        if not valid_losses:
            return (0, 0.1)
        
        min_loss = min(valid_losses)
        max_loss = max(valid_losses)
        
        # å¦‚æœæ‰€æœ‰æå¤±å€¼éƒ½ç›¸åŒï¼Œæ·»åŠ ä¸€äº›é‚Šè·
        if min_loss == max_loss:
            margin = max(abs(min_loss) * 0.1, 0.01)
            return (max(0, min_loss - margin), max_loss + margin)
        
        # è¨ˆç®—ç¯„åœå’Œé‚Šè·
        loss_range = max_loss - min_loss
        margin = loss_range * self.loss_scale_margin
        
        # ç¢ºä¿ä¸‹é™ä¸æœƒå°æ–¼ 0ï¼ˆå°æ–¼å¤§å¤šæ•¸æå¤±å‡½æ•¸ï¼‰
        y_min = max(0, min_loss - margin)
        y_max = max_loss + margin
        
        return (y_min, y_max)

    def _calculate_dynamic_tick_interval(self, y_min, y_max, target_ticks=8):
        """è¨ˆç®—å‹•æ…‹çš„åˆ»åº¦é–“è·"""
        if y_max <= y_min:
            return 0.01
        
        range_val = y_max - y_min
        raw_interval = range_val / target_ticks
        
        # å°‡é–“è·èª¿æ•´ç‚º"å¥½çœ‹"çš„æ•¸å­—
        magnitude = 10 ** np.floor(np.log10(raw_interval))
        normalized = raw_interval / magnitude
        
        if normalized <= 1:
            nice_interval = 1
        elif normalized <= 2:
            nice_interval = 2
        elif normalized <= 5:
            nice_interval = 5
        else:
            nice_interval = 10
            
        return nice_interval * magnitude

    def _initialize_plot(self):
        """åˆå§‹åŒ–åœ–è¡¨"""
        try:
            # å‰µå»ºä¸€å€‹ Figure å’Œå…©å€‹å­åœ– (Axes)
            self.fig, self.axes = plt.subplots(2, 1, figsize=(15, 10))
            self.fig.suptitle('True Multi-Scale Training Progress', fontsize=16)
            plt.ion()  # é–‹å•Ÿäº’å‹•æ¨¡å¼
        except Exception as e:
            logger.warning(f"Could not initialize plot: {e}")
            self.fig = None

    def add_metric(self, epoch, step, ami_score, loss, learning_rate=None):
        """æ·»åŠ æ–°çš„æŒ‡æ¨™è¨˜éŒ„"""
        self.metrics_data['epoch'].append(epoch)
        self.metrics_data['step'].append(step)
        self.metrics_data['timestamp'].append(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))
        self.metrics_data['AMI_score'].append(ami_score)
        self.metrics_data['loss'].append(loss)
        self.metrics_data['learning_rate'].append(learning_rate)
        
        self.save_to_excel()
        
        if self.fig is not None:
            self.update_plot()

    def save_to_excel(self):
        """ä¿å­˜æŒ‡æ¨™åˆ°Excelæ–‡ä»¶"""
        try:
            df = pd.DataFrame(self.metrics_data)
            df.to_excel(self.save_path, index=False)
            logger.info(f"Metrics saved to {self.save_path}")
        except Exception as e:
            logger.warning(f"Could not save metrics to Excel: {e}")

    def update_plot(self):
        """æ›´æ–°ç·šåœ–"""
        if len(self.metrics_data['epoch']) == 0 or self.fig is None:
            return

        ax1, ax2 = self.axes
        
        try:
            ax1.clear()
            ax2.clear()
            
            steps = self.metrics_data['step']
            ami_scores = self.metrics_data['AMI_score']
            losses = self.metrics_data['loss']
            
            # --- ä¸Šæ–¹åœ–è¡¨: AMI Score ---
            ax1.plot(steps, ami_scores, 'b-', marker='o', linewidth=2, markersize=4, label='AMI Score')
            
            # Add score annotations above each point
            for i, (step, score) in enumerate(zip(steps, ami_scores)):
                ax1.annotate(f'{score:.3f}', 
                           xy=(step, score), 
                           xytext=(0, 10),  # 10 points above the point
                           textcoords='offset points',
                           ha='center', va='bottom',
                           fontsize=8, 
                           bbox=dict(boxstyle='round,pad=0.3', facecolor='lightblue', alpha=0.7))
            
            ax1.set_ylabel('AMI Score')
            ax1.set_title('Validation AMI Score')
            ax1.grid(True, which='both', linestyle='--', linewidth=0.5)
            ax1.set_ylim(self.y_lim_ami) # è¨­å®š Y è»¸ç¯„åœ
            if self.y_tick_interval_ami:
                ax1.yaxis.set_major_locator(mticker.MultipleLocator(self.y_tick_interval_ami))
            
            # --- ä¸‹æ–¹åœ–è¡¨: Training Loss ---
            ax2.plot(steps, losses, 'r-', marker='s', linewidth=2, markersize=4, label='Multi-Scale Loss')
            
            # Add loss annotations above each point
            for step, loss in zip(steps, losses):
                if loss is not None and loss != 0:
                    ax2.annotate(f'{loss:.4f}', 
                               xy=(step, loss), 
                               xytext=(0, 10),  # 10 points above the point
                               textcoords='offset points',
                               ha='center', va='bottom',
                               fontsize=8, 
                               bbox=dict(boxstyle='round,pad=0.3', facecolor='lightcoral', alpha=0.7))
            
            ax2.set_xlabel('Training Step')
            ax2.set_ylabel('Loss')
            ax2.set_title('Multi-Scale Training Loss')
            ax2.grid(True, which='both', linestyle='--', linewidth=0.5)
            
            # å‹•æ…‹è¨­å®šæå¤±åœ–çš„ Y è»¸ç¯„åœå’Œåˆ»åº¦
            if self.y_lim_loss is None:
                # ä½¿ç”¨å‹•æ…‹ç¸®æ”¾
                y_min_loss, y_max_loss = self._calculate_dynamic_loss_scale(losses)
                ax2.set_ylim(y_min_loss, y_max_loss)
                
                # å‹•æ…‹è¨­å®šåˆ»åº¦é–“è·
                if self.y_tick_interval_loss is None:
                    tick_interval = self._calculate_dynamic_tick_interval(y_min_loss, y_max_loss)
                    ax2.yaxis.set_major_locator(mticker.MultipleLocator(tick_interval))
                else:
                    ax2.yaxis.set_major_locator(mticker.MultipleLocator(self.y_tick_interval_loss))
            else:
                # ä½¿ç”¨å›ºå®šç¸®æ”¾
                ax2.set_ylim(self.y_lim_loss)
                if self.y_tick_interval_loss:
                    ax2.yaxis.set_major_locator(mticker.MultipleLocator(self.y_tick_interval_loss))

            # --- å…±ç”¨ X è»¸è¨­å®š ---
            for ax in self.axes:
                ax.legend()
                if self.x_tick_interval:
                    ax.xaxis.set_major_locator(mticker.MultipleLocator(self.x_tick_interval))
            
            self.fig.tight_layout(rect=[0, 0.03, 1, 0.95])
            plt.pause(0.1)
            
        except Exception as e:
            logger.warning(f"Could not update plot: {e}")

    def save_plot(self, save_path='./training_progress.png'):
        """ä¿å­˜åœ–è¡¨"""
        if self.fig is not None:
            try:
                self.fig.savefig(save_path, dpi=300, bbox_inches='tight')
                logger.info(f"Plot saved to {save_path}")
            except Exception as e:
                logger.warning(f"Could not save plot: {e}")

    def close(self):
        """é—œé–‰åœ–è¡¨"""
        if self.fig is not None:
            try:
                plt.ioff()
                plt.close(self.fig)
            except Exception as e:
                logger.warning(f"Could not close plot: {e}")

class CustomTrainingCallback:
    """
    è‡ªå®šç¾©è¨“ç·´å›èª¿ï¼Œç”¨æ–¼å¯¦æ™‚ AMI è¿½è¹¤å’Œå¯è¦–åŒ– (å¾ train.py å°å…¥)
    """
    def __init__(self, metrics_tracker, model, train_loss, val_sentences, val_labels):
        self.metrics_tracker = metrics_tracker
        self.model = model
        self.train_loss = train_loss
        self.val_sentences = val_sentences
        self.val_labels = val_labels
        self.step_count = 0
        self.current_epoch = 0
        self.current_loss = 0.0
        self.loss_history = []
        
        # è¨­ç½® loss è¿½è¹¤
        self._setup_loss_tracking()

    def _setup_loss_tracking(self):
        """è¨­ç½® loss è¿½è¹¤é‰¤å­"""
        try:
            if hasattr(self.train_loss, 'forward'):
                original_forward = self.train_loss.forward
                
                def loss_hook(*args, **kwargs):
                    loss = original_forward(*args, **kwargs)
                    # æ•ç² loss å€¼
                    if hasattr(loss, 'item'):
                        self.current_loss = loss.item()
                    elif isinstance(loss, (float, int)):
                        self.current_loss = float(loss)
                    else:
                        self.current_loss = float(loss)
                    
                    self.loss_history.append(self.current_loss)
                    # ä¿æŒæœ€è¿‘ 100 å€‹ loss å€¼
                    if len(self.loss_history) > 100:
                        self.loss_history.pop(0)
                    return loss
                
                self.train_loss.forward = loss_hook
                logger.info("Loss tracking hook installed successfully")
            else:
                logger.warning("Could not install loss tracking hook - no forward method found")
        except Exception as e:
            logger.warning(f"Could not install loss tracking hook: {e}")

    def __call__(self, score, epoch, steps):
        """åœ¨è¨“ç·´éç¨‹ä¸­è¢«èª¿ç”¨ - å¯¦æ™‚è¨ˆç®—AMIä¸¦æ›´æ–°åœ–è¡¨"""
        self.step_count = steps
        self.current_epoch = epoch
        
        # ç²å–ç•¶å‰å­¸ç¿’ç‡
        current_lr = self._get_current_learning_rate()
        
        # è¨ˆç®—å¯¦æ™‚ AMI åˆ†æ•¸
        ami_score = self._evaluate_clustering_performance()
        
        # ç²å–å¹³å‡ lossï¼ˆä½¿ç”¨æœ€è¿‘çš„ loss å€¼é€²è¡Œå¹³æ»‘ï¼‰
        if self.loss_history:
            recent_losses = self.loss_history[-10:]  # æœ€è¿‘ 10 å€‹ loss
            avg_loss = sum(recent_losses) / len(recent_losses)
        else:
            avg_loss = self.current_loss
        
        # æ›´æ–°æŒ‡æ¨™è¿½è¹¤å™¨ - å¯¦æ™‚æ›´æ–°åœ–è¡¨ï¼
        if self.metrics_tracker:
            self.metrics_tracker.add_metric(
                epoch=epoch,
                step=steps,
                ami_score=ami_score,
                loss=avg_loss,
                learning_rate=current_lr
            )
        
        logger.info(f"Callback - Epoch {epoch:.2f}, Step {steps}: AMI = {ami_score:.4f}, Loss = {avg_loss:.5f}, LR = {current_lr:.2e}")

    def _evaluate_clustering_performance(self):
        """å¤šæ–¹æ³•èšé¡è©•ä¼°ï¼Œè‡ªå‹•é¸æ“‡æœ€ä½³æ–¹æ³•ï¼ˆå¯¦æ™‚ç‰ˆæœ¬ï¼‰"""
        try:
            # ç”ŸæˆåµŒå…¥
            embeddings = self.model.encode(self.val_sentences, show_progress_bar=False)
            
            clustering_results = {}
            
            # 1. DBSCAN èšé¡
            try:
                dbscan = DBSCAN(eps=0.095, min_samples=2, metric='cosine')
                dbscan_labels = dbscan.fit_predict(embeddings)
                
                # è™•ç†å™ªè²é»
                max_label = max(dbscan_labels) if len(dbscan_labels) > 0 and max(dbscan_labels) >= 0 else -1
                noise_mask = dbscan_labels == -1
                n_noise = np.sum(noise_mask)
                if n_noise > 0:
                    dbscan_labels[noise_mask] = range(max_label + 1, max_label + 1 + n_noise)
                
                dbscan_ami = adjusted_mutual_info_score(self.val_labels, dbscan_labels)
                clustering_results['dbscan'] = dbscan_ami
            except Exception as e:
                logger.warning(f"DBSCAN clustering failed: {e}")
                clustering_results['dbscan'] = 0.0
            
            # 2. å±¤æ¬¡èšé¡ï¼ˆå¿«é€Ÿç‰ˆæœ¬ - åªæ¸¬è©¦å°‘æ•¸å¹¾å€‹åƒæ•¸ï¼‰
            n_samples = len(embeddings)
            n_unique_labels = len(set(self.val_labels))
            max_clusters = min(n_samples - 1, 50)
            
            # é¸æ“‡2-3å€‹ä»£è¡¨æ€§çš„clusteræ•¸é‡é€²è¡Œå¿«é€Ÿæ¸¬è©¦
            quick_cluster_candidates = []
            for offset in [0, 5]:  # åªæ¸¬è©¦2å€‹å€¼ä»¥åŠ å¿«é€Ÿåº¦
                n_clusters = n_unique_labels + offset
                if 2 <= n_clusters <= max_clusters:
                    quick_cluster_candidates.append(n_clusters)
            
            # å¦‚æœæ²’æœ‰åˆé©çš„å€™é¸ï¼Œä½¿ç”¨ä¿å®ˆé¸æ“‡
            if not quick_cluster_candidates:
                quick_cluster_candidates = [min(max_clusters, max(2, n_unique_labels))]
            
            for n_clusters in quick_cluster_candidates:
                try:
                    hier = AgglomerativeClustering(n_clusters=n_clusters, linkage='average', metric='cosine')
                    hier_labels = hier.fit_predict(embeddings)
                    hier_ami = adjusted_mutual_info_score(self.val_labels, hier_labels)
                    clustering_results[f'hierarchical_{n_clusters}'] = hier_ami
                except Exception as e:
                    logger.warning(f"Hierarchical clustering with {n_clusters} clusters failed: {e}")
                    clustering_results[f'hierarchical_{n_clusters}'] = 0.0
            
            # æ‰¾åˆ°æœ€ä½³çµæœ
            if clustering_results:
                best_method = max(clustering_results.items(), key=lambda x: x[1])
                best_ami = best_method[1]
                best_method_name = best_method[0]
                
                # æ¯2500æ­¥æ‰“å°ä¸€æ¬¡è©³ç´°çµæœï¼Œé¿å…æ—¥èªŒéå¤š
                if self.step_count % 2500 == 0:  # æ¯2500æ­¥æ‰“å°ä¸€æ¬¡è©³ç´°çµæœ
                    logger.info(f"Multi-method clustering results: {clustering_results}")
                    logger.info(f"Best method: {best_method_name} with AMI {best_ami:.4f}")
                
                return best_ami
            else:
                return 0.0
            
        except Exception as e:
            logger.warning(f"Multi-method clustering evaluation failed: {e}")
            return 0.0

    def _get_current_learning_rate(self):
        """ç²å–ç•¶å‰å­¸ç¿’ç‡"""
        try:
            if hasattr(self.model, 'optimizer') and self.model.optimizer:
                optimizer = self.model.optimizer
                if hasattr(optimizer, 'param_groups') and len(optimizer.param_groups) > 0:
                    return optimizer.param_groups[0]['lr']
        except AttributeError:
            pass
        
        return 8e-6  # è¿”å›é»˜èªå€¼

class TrueMultiScaleLoss(nn.Module):
    """
    çœŸæ­£çš„å¤šå°ºåº¦æå¤±å‡½æ•¸ï¼Œç¢ºä¿ä¸åŒå°ºåº¦å­¸ç¿’ä¸åŒç‰¹å¾µ
    """
    
    def __init__(self, model, scale=20.0, orthogonal_weight=0.1, consistency_weight=0.2):
        super().__init__()
        self.model = model
        self.scale = scale
        self.orthogonal_weight = orthogonal_weight
        self.consistency_weight = consistency_weight
        
        # æ ¸å¿ƒæå¤±å‡½æ•¸
        self.core_loss = MultipleNegativesRankingLoss(model=model, scale=scale)
        
        # ç²å–æ¨¡å‹çš„ä¸åŒå±¤ç‰¹å¾µ
        self.embedding_dim = model.get_sentence_embedding_dimension()
        
        # å…¨å±€èªç¾©æŠ•å½±ï¼ˆé—œæ³¨æ•´é«”è¦å‰‡é¡åˆ¥ï¼‰- æ¸›å°‘ç¶­åº¦ä»¥ç¯€çœå…§å­˜
        self.global_proj = nn.Sequential(
            nn.Linear(self.embedding_dim, 256),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(256, 192),
            nn.LayerNorm(192)
        )
        
        # å±€éƒ¨æ¨¡å¼æŠ•å½±ï¼ˆé—œæ³¨å±¤é–“é—œä¿‚å’Œç©ºé–“ç´„æŸï¼‰
        self.local_proj = nn.Sequential(
            nn.Linear(self.embedding_dim, 192),
            nn.ReLU(), 
            nn.Dropout(0.1),
            nn.Linear(192, 128),
            nn.LayerNorm(128)
        )
        
        # ç´°ç²’åº¦æŠ•å½±ï¼ˆé—œæ³¨å…·é«”åƒæ•¸å’Œæ•¸å€¼ï¼‰
        self.fine_proj = nn.Sequential(
            nn.Linear(self.embedding_dim, 128),
            nn.ReLU(),
            nn.Dropout(0.1), 
            nn.Linear(128, 96),
            nn.LayerNorm(96)
        )
        
        # æ³¨æ„åŠ›æ©Ÿåˆ¶å¼•å°ä¸åŒå°ºåº¦é—œæ³¨ä¸åŒæ–¹é¢ - æ¸›å°‘headsæ•¸é‡ä»¥ç¯€çœå…§å­˜
        self.global_attention = self._create_attention_head(self.embedding_dim, "global")
        self.local_attention = self._create_attention_head(self.embedding_dim, "local")  
        self.fine_attention = self._create_attention_head(self.embedding_dim, "fine")
        
        # ç§»å‹•åˆ°èˆ‡æ¨¡å‹ç›¸åŒçš„è¨­å‚™
        self.to(model.device)
        
    def _create_attention_head(self, dim, scale_type):
        """ç‚ºä¸åŒå°ºåº¦å‰µå»ºå°ˆé–€çš„æ³¨æ„åŠ›é ­ - æ¸›å°‘headsæ•¸é‡ä»¥ç¯€çœå…§å­˜"""
        if scale_type == "global":
            # å…¨å±€æ³¨æ„åŠ›ï¼šé—œæ³¨è¦å‰‡é¡å‹é—œéµè©
            return nn.MultiheadAttention(dim, num_heads=4, dropout=0.1, batch_first=True)
        elif scale_type == "local":
            # å±€éƒ¨æ³¨æ„åŠ›ï¼šé—œæ³¨å±¤åå’Œç©ºé–“é—œä¿‚
            return nn.MultiheadAttention(dim, num_heads=2, dropout=0.1, batch_first=True)
        else:  # fine
            # ç´°ç²’åº¦æ³¨æ„åŠ›ï¼šé—œæ³¨æ•¸å€¼å’Œæ¸¬é‡å–®ä½
            return nn.MultiheadAttention(dim, num_heads=1, dropout=0.1, batch_first=True)
    
    def _apply_scale_specific_attention(self, embeddings, attention_head, scale_type):
        """æ‡‰ç”¨å°ºåº¦ç‰¹å®šçš„æ³¨æ„åŠ›æ©Ÿåˆ¶"""
        # å°‡ sentence embedding è¦–ç‚ºå–®å€‹ token çš„åºåˆ—
        # åœ¨å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œé€™è£¡å¯ä»¥ç²å– token-level çš„ç‰¹å¾µ
        emb_expanded = embeddings.unsqueeze(1)  # [batch, 1, dim]
        
        # è‡ªæ³¨æ„åŠ›
        attended, _ = attention_head(emb_expanded, emb_expanded, emb_expanded)
        return attended.squeeze(1)  # [batch, dim]
    
    def _orthogonal_loss(self):
        """æ­£äº¤æ€§æå¤±ï¼šç¢ºä¿ä¸åŒæŠ•å½±å­¸ç¿’ä¸åŒç‰¹å¾µ - ç”¨æˆ¶æ”¹é€²ç‰ˆæœ¬"""
        w_global = self.global_proj[-2].weight
        w_local = self.local_proj[-2].weight  
        w_fine = self.fine_proj[-2].weight
        
        w_global_norm = F.normalize(w_global, p=2, dim=1)
        w_local_norm = F.normalize(w_local, p=2, dim=1)
        w_fine_norm = F.normalize(w_fine, p=2, dim=1)
        
        def compute_ortho_loss(w1, w2):
            min_out_dim = min(w1.shape[0], w2.shape[0])
            min_in_dim = min(w1.shape[1], w2.shape[1])
            w1_final = w1[:min_out_dim, :min_in_dim]
            w2_final = w2[:min_out_dim, :min_in_dim]
            ortho_loss = torch.pow(F.cosine_similarity(w1_final.flatten(), w2_final.flatten(), dim=0), 2)
            return ortho_loss
        
        ortho_loss_global_local = compute_ortho_loss(w_global_norm, w_local_norm)
        ortho_loss_global_fine = compute_ortho_loss(w_global_norm, w_fine_norm)
        ortho_loss_local_fine = compute_ortho_loss(w_local_norm, w_fine_norm)
        
        total_ortho_loss = (ortho_loss_global_local + ortho_loss_global_fine + ortho_loss_local_fine) / 3
        return total_ortho_loss
    
    def _consistency_loss(self, global_emb, local_emb, fine_emb):
        """ä¸€è‡´æ€§æå¤±ï¼šç¢ºä¿ä¸åŒå°ºåº¦åœ¨ç›¸åŒæ¨£æœ¬ä¸Šä¿æŒä¸€è‡´çš„ç›¸å°é—œä¿‚"""
        batch_size = global_emb.shape[0]
        
        if batch_size < 2:
            return torch.tensor(0.0, device=global_emb.device)
        
        # è¨ˆç®—æ¯å€‹å°ºåº¦å…§çš„ç›¸ä¼¼åº¦çŸ©é™£
        global_sim = F.cosine_similarity(global_emb.unsqueeze(1), global_emb.unsqueeze(0), dim=2)
        local_sim = F.cosine_similarity(local_emb.unsqueeze(1), local_emb.unsqueeze(0), dim=2)
        fine_sim = F.cosine_similarity(fine_emb.unsqueeze(1), fine_emb.unsqueeze(0), dim=2)
        
        # è¨ˆç®—ç›¸ä¼¼åº¦çŸ©é™£é–“çš„ä¸€è‡´æ€§
        consistency_loss = (
            F.mse_loss(global_sim, local_sim) + 
            F.mse_loss(global_sim, fine_sim) + 
            F.mse_loss(local_sim, fine_sim)
        ) / 3
        
        return consistency_loss
    
    def _scale_specific_features(self, embeddings, sentences):
        """æ ¹æ“šæ–‡æœ¬å…§å®¹æå–å°ºåº¦ç‰¹å®šç‰¹å¾µ"""
        batch_size = embeddings.shape[0]
        
        # å…¨å±€ç‰¹å¾µï¼šè¦å‰‡é¡å‹æŒ‡ç¤º
        global_features = []
        # å±€éƒ¨ç‰¹å¾µï¼šå±¤é—œä¿‚æŒ‡ç¤º  
        local_features = []
        # ç´°ç²’åº¦ç‰¹å¾µï¼šæ•¸å€¼æŒ‡ç¤º
        fine_features = []
        
        for sentence in sentences:
            # å…¨å±€èªç¾©ç‰¹å¾µï¼ˆè¦å‰‡é¡å‹ï¼‰
            global_feat = self._extract_global_features(sentence)
            global_features.append(global_feat)
            
            # å±€éƒ¨ç‰¹å¾µï¼ˆå±¤é—œä¿‚ï¼‰
            local_feat = self._extract_local_features(sentence)
            local_features.append(local_feat)
            
            # ç´°ç²’åº¦ç‰¹å¾µï¼ˆæ•¸å€¼åƒæ•¸ï¼‰
            fine_feat = self._extract_fine_features(sentence)
            fine_features.append(fine_feat)
        
        global_features = torch.tensor(global_features, device=embeddings.device, dtype=torch.float32)
        local_features = torch.tensor(local_features, device=embeddings.device, dtype=torch.float32)
        fine_features = torch.tensor(fine_features, device=embeddings.device, dtype=torch.float32)
        
        return global_features, local_features, fine_features
    
    def _extract_global_features(self, sentence):
        """æå–å…¨å±€èªç¾©ç‰¹å¾µï¼ˆè¦å‰‡é¡å‹ï¼‰"""
        sentence_lower = sentence.lower()
        
        # è¦å‰‡é¡å‹ç‰¹å¾µ
        spacing_feat = 1.0 if 'spacing' in sentence_lower or 'space' in sentence_lower else 0.0
        width_feat = 1.0 if 'width' in sentence_lower or 'wide' in sentence_lower else 0.0  
        enclosure_feat = 1.0 if 'enclosure' in sentence_lower or 'enclose' in sentence_lower else 0.0
        overlap_feat = 1.0 if 'overlap' in sentence_lower else 0.0
        minimum_feat = 1.0 if 'minimum' in sentence_lower or 'min' in sentence_lower else 0.0
        maximum_feat = 1.0 if 'maximum' in sentence_lower or 'max' in sentence_lower else 0.0
        
        return [spacing_feat, width_feat, enclosure_feat, overlap_feat, minimum_feat, maximum_feat]
    
    def _extract_local_features(self, sentence):
        """æå–å±€éƒ¨ç‰¹å¾µï¼ˆå±¤é—œä¿‚ï¼‰"""
        sentence_upper = sentence.upper()
        
        # å±¤é¡å‹ç‰¹å¾µ
        metal_feat = 1.0 if 'METAL' in sentence_upper else 0.0
        poly_feat = 1.0 if 'POLY' in sentence_upper else 0.0
        diffusion_feat = 1.0 if 'DIFFUSION' in sentence_upper else 0.0
        contact_feat = 1.0 if 'CONTACT' in sentence_upper or 'VIA' in sentence_upper else 0.0
        well_feat = 1.0 if 'WELL' in sentence_upper else 0.0
        
        return [metal_feat, poly_feat, diffusion_feat, contact_feat, well_feat]
    
    def _extract_fine_features(self, sentence):
        """æå–ç´°ç²’åº¦ç‰¹å¾µï¼ˆæ•¸å€¼åƒæ•¸ï¼‰"""
        # æ•¸å€¼ç‰¹å¾µ
        numbers = re.findall(r'\d+\.?\d*', sentence)
        has_numbers = 1.0 if numbers else 0.0
        
        # å–®ä½ç‰¹å¾µ
        has_micron = 1.0 if 'um' in sentence.lower() or 'micron' in sentence.lower() else 0.0
        has_nano = 1.0 if 'nm' in sentence.lower() or 'nano' in sentence.lower() else 0.0
        
        # æ•¸å€¼å¤§å°ç‰¹å¾µï¼ˆç²—ç•¥åˆ†é¡ï¼‰
        if numbers:
            max_num = max([float(n) for n in numbers])
            large_num = 1.0 if max_num > 10 else 0.0
            small_num = 1.0 if max_num < 1 else 0.0
        else:
            large_num = 0.0
            small_num = 0.0
            
        return [has_numbers, has_micron, has_nano, large_num, small_num]
    
    def forward(self, sentence_features, labels):
        """å¤šå°ºåº¦å±¤æ¬¡åŒ–æå¤±è¨ˆç®—"""
        # ç²å–åŸºç¤ embeddings
        reps = [self.model(sentence_feature)['sentence_embedding'] for sentence_feature in sentence_features]
        anchor_emb, positive_emb = reps[0], reps[1]
        
        # æ ¸å¿ƒæå¤±ï¼ˆæ¨™æº– Multiple Negatives Rankingï¼‰
        core_loss_value = self.core_loss(sentence_features, labels)
        
        # æ‡‰ç”¨å°ºåº¦ç‰¹å®šæ³¨æ„åŠ›ï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼Œä¸ä¾è³´æ–‡æœ¬æå–ï¼‰
        global_anchor = self._apply_scale_specific_attention(anchor_emb, self.global_attention, "global")
        global_positive = self._apply_scale_specific_attention(positive_emb, self.global_attention, "global")
        
        local_anchor = self._apply_scale_specific_attention(anchor_emb, self.local_attention, "local")
        local_positive = self._apply_scale_specific_attention(positive_emb, self.local_attention, "local")
        
        fine_anchor = self._apply_scale_specific_attention(anchor_emb, self.fine_attention, "fine")
        fine_positive = self._apply_scale_specific_attention(positive_emb, self.fine_attention, "fine")
        
        # å°ºåº¦ç‰¹å®šæŠ•å½±
        global_anchor_proj = F.normalize(self.global_proj(global_anchor), p=2, dim=1)
        global_positive_proj = F.normalize(self.global_proj(global_positive), p=2, dim=1)
        
        local_anchor_proj = F.normalize(self.local_proj(local_anchor), p=2, dim=1)
        local_positive_proj = F.normalize(self.local_proj(local_positive), p=2, dim=1)
        
        fine_anchor_proj = F.normalize(self.fine_proj(fine_anchor), p=2, dim=1)
        fine_positive_proj = F.normalize(self.fine_proj(fine_positive), p=2, dim=1)
        
        # å°ºåº¦ç‰¹å®šå°æ¯”æå¤±
        global_loss = 1.0 - F.cosine_similarity(global_anchor_proj, global_positive_proj, dim=1).mean()
        local_loss = 1.0 - F.cosine_similarity(local_anchor_proj, local_positive_proj, dim=1).mean()
        fine_loss = 1.0 - F.cosine_similarity(fine_anchor_proj, fine_positive_proj, dim=1).mean()
        
        # æ­£äº¤æ€§æå¤±ï¼šç¢ºä¿ä¸åŒæŠ•å½±å­¸ç¿’ä¸åŒç‰¹å¾µ
        orthogonal_loss = self._orthogonal_loss()
        
        # ä¸€è‡´æ€§æå¤±ï¼šç¢ºä¿ä¸åŒå°ºåº¦ä¿æŒç›¸å°é—œä¿‚ä¸€è‡´
        consistency_loss = self._consistency_loss(
            global_anchor_proj, local_anchor_proj, fine_anchor_proj
        )
        
        # çµ„åˆæå¤±
        total_loss = (
            core_loss_value +  # ä¸»è¦æå¤±
            0.3 * global_loss +  # å…¨å±€èªç¾©æå¤±
            0.25 * local_loss +  # å±€éƒ¨é—œä¿‚æå¤±  
            0.2 * fine_loss +   # ç´°ç²’åº¦æå¤±
            self.orthogonal_weight * orthogonal_loss +  # æ­£äº¤æ€§ç´„æŸ
            self.consistency_weight * consistency_loss   # ä¸€è‡´æ€§ç´„æŸ
        )
        
        return total_loss

class TrueMultiScaleTrainer:
    """çœŸæ­£çš„å¤šå°ºåº¦è¨“ç·´å™¨"""
    
    def __init__(self, model_name="sentence-transformers/all-mpnet-base-v2"):
        self.model_name = model_name
        self.model = None
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.metrics_tracker = None
        logger.info(f"Using device: {self.device}")
        
    def load_model(self):
        """è¼‰å…¥é è¨“ç·´æ¨¡å‹"""
        logger.info(f"Loading model: {self.model_name}")
        try:
            self.model = SentenceTransformer(self.model_name)
            self.model.to(self.device)
            logger.info("Model loaded successfully")
        except Exception as e:
            logger.error(f"Error loading model: {e}")
            raise
    
    def prepare_data(self, path4ruleGT, model_path, enhanced_triplet=True, difficulty_strategy='mixed'):
        """æº–å‚™è¨“ç·´æ•¸æ“šï¼Œä½¿ç”¨èˆ‡ train.py å®Œå…¨ç›¸åŒçš„æ–¹æ³•"""
        logger.info("Preparing training data...")
        
        # ä½¿ç”¨èˆ‡ train.py ç›¸åŒçš„æ–¹æ³•
        result = main_generate_train_data_with_hard_mining(
            train_ratio=0.7,
            val_ratio=0.15,
            test_ratio=0.15,
            max_triplets_per_anchor=24,
            difficulty_ratio=0.6,
            path4ruleGT=path4ruleGT,        
            hard_negative_ratio=0.7,
            similarity_method='sentence_transformer',
            model_path=model_path,
            top_k_hard_negatives=10
        )
        
        if result is None or result[0] is None:
            raise ValueError("No training examples generated")
        
        train_triplets, dataset_dict = result
        logger.info(f"Generated {len(train_triplets)} training triplets.")
        
        # å‰µå»ºé©—è­‰ä¸‰å…ƒçµ„ï¼Œèˆ‡ train.py å®Œå…¨ç›¸åŒ
        val_triplets, test_triplets = create_validation_triplets(dataset_dict)
        logger.info(f"Generated {len(val_triplets)} validation triplets.")
        
        # æå–é©—è­‰å¥å­å’Œæ¨™ç±¤ï¼Œèˆ‡ train.py å®Œå…¨ç›¸åŒ
        val_df = dataset_dict['validation'].to_pandas()
        val_sentences = val_df['description'].tolist()
        
        # ç¢ºä¿æ¨™ç±¤æ˜¯æ•´æ•¸
        le = LabelEncoder()
        val_labels = le.fit_transform(val_df['group_id'].tolist())
        
        logger.info(f"Prepared validation set with {len(val_sentences)} sentences and {len(set(val_labels))} unique labels.")
        return train_triplets, val_triplets, val_sentences, val_labels
    
    def train_true_multi_scale_model(self, train_examples, val_data, val_sentences, val_labels, 
                                   output_path, epochs=4, batch_size=8, learning_rate=1e-5):
        """çœŸæ­£çš„å¤šå°ºåº¦è¨“ç·´"""
        logger.info("=== True Multi-Scale Training Started ===")
        logger.info(f"Training samples: {len(train_examples)}")
        logger.info(f"Validation samples: {len(val_data) if val_data else 0}")
        logger.info(f"Epochs: {epochs}")
        logger.info(f"Batch size: {batch_size}")
        logger.info(f"Learning rate: {learning_rate}")
        
        # åˆå§‹åŒ–æŒ‡æ¨™è¿½è¹¤å™¨
        date_str = datetime.now().strftime('%Y%m%d_%H%M%S')
        metrics_path = f'./true_multi_scale_training_metrics_{date_str}.xlsx'
        plot_path = f'./training_metrics_plot_{date_str}.png'
        
        self.metrics_tracker = MetricsTracker(
            save_path=metrics_path,
            y_lim_ami=(0.3, 1.05),
            y_lim_loss=None,  # å‹•æ…‹ç¸®æ”¾
        )
        
        # å‰µå»º DataLoader
        train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=batch_size)
        
        # çœŸæ­£çš„å¤šå°ºåº¦æå¤±å‡½æ•¸
        train_loss = TrueMultiScaleLoss(
            model=self.model,
            scale=20.0,
            orthogonal_weight=0.1,  # æ­£äº¤æ€§ç´„æŸæ¬Šé‡
            consistency_weight=0.2   # ä¸€è‡´æ€§ç´„æŸæ¬Šé‡
        )
        
        # å‰µå»ºè‡ªå®šç¾©è¨“ç·´å›èª¿ - å¯¦ç¾å¯¦æ™‚ AMI è¿½è¹¤
        training_callback = CustomTrainingCallback(
            metrics_tracker=self.metrics_tracker,
            model=self.model,
            train_loss=train_loss,
            val_sentences=val_sentences,
            val_labels=val_labels
        )
        
        # è¨“ç·´åƒæ•¸
        training_args = {
            'epochs': epochs,
            'scheduler': 'WarmupLinear',
            'warmup_steps': int(0.1 * len(train_dataloader) * epochs),
            'optimizer_class': torch.optim.AdamW,
            'optimizer_params': {'lr': learning_rate, 'weight_decay': 0.01},
            'evaluation_steps': len(train_dataloader) // 4,
            'save_best_model': True,
            'show_progress_bar': True,
            'callback': training_callback  # æ·»åŠ å¯¦æ™‚AMIè¿½è¹¤å›èª¿
        }
        
        # é©—è­‰è©•ä¼°å™¨
        evaluator = None
        if val_data:
            val_dataloader = DataLoader(val_data, shuffle=False, batch_size=batch_size)
            evaluator = evaluation.TripletEvaluator.from_input_examples(
                val_data, batch_size=batch_size, name='true_multi_scale_val'
            )
        
        # è¨“ç·´æ¨¡å‹
        start_time = time.time()
        
        # åœ¨è¨“ç·´é–‹å§‹å‰è©•ä¼°ä¸€æ¬¡åˆå§‹æ€§èƒ½
        initial_ami = self.evaluate_clustering_performance(val_sentences, val_labels)
        if self.metrics_tracker:
            self.metrics_tracker.add_metric(
                epoch=0, step=0, ami_score=initial_ami, loss=0.0, learning_rate=learning_rate
            )
        
        self.model.fit(
            train_objectives=[(train_dataloader, train_loss)],
            evaluator=evaluator,
            **training_args
        )
        
        training_time = time.time() - start_time
        logger.info(f"Training completed in {training_time:.2f} seconds")
        
        # ä¿å­˜æ¨¡å‹
        logger.info(f"Saving true multi-scale model to: {output_path}")
        self.model.save(output_path)
        
        # æœ€çµ‚è©•ä¼°
        final_ami = self.evaluate_clustering_performance(val_sentences, val_labels)
        
        # ä¿å­˜æœ€çµ‚æŒ‡æ¨™å’Œåœ–è¡¨
        if self.metrics_tracker:
            self.metrics_tracker.add_metric(
                epoch=epochs, step=len(train_dataloader) * epochs, 
                ami_score=final_ami, loss=0.0, learning_rate=learning_rate
            )
            self.metrics_tracker.save_plot(plot_path)
            logger.info(f"Training metrics and plot saved to: {metrics_path}, {plot_path}")
            self.metrics_tracker.close()
        
        return final_ami
    
    def evaluate_clustering_performance(self, sentences, labels):
        """è©•ä¼°èšé¡æ€§èƒ½"""
        logger.info("Evaluating clustering performance...")
        
        # ç”ŸæˆåµŒå…¥
        embeddings = self.model.encode(sentences, show_progress_bar=True)
        
        # æ¸¬è©¦å¤šç¨®èšé¡æ–¹æ³•
        clustering_results = {}
        
        # DBSCAN 
        dbscan = DBSCAN(eps=0.095, min_samples=2, metric='cosine')
        dbscan_labels = dbscan.fit_predict(embeddings)
        
        # æ‡‰ç”¨å€‹åˆ¥å™ªè²ç­–ç•¥
        max_label = max(dbscan_labels) if len(dbscan_labels) > 0 and max(dbscan_labels) >= 0 else -1
        noise_mask = dbscan_labels == -1
        n_noise = np.sum(noise_mask)
        if n_noise > 0:
            dbscan_labels[noise_mask] = range(max_label + 1, max_label + 1 + n_noise)
        
        dbscan_ami = adjusted_mutual_info_score(labels, dbscan_labels)
        clustering_results['dbscan'] = dbscan_ami
        
        # å±¤æ¬¡èšé¡è®Šé«” - å‹•æ…‹èª¿æ•´clusteræ•¸é‡
        n_samples = len(embeddings)
        max_clusters = n_samples - 1
        n_unique_labels = len(set(labels))
        
        # æ¸¬è©¦ä¸åŒçš„clusteræ•¸é‡ï¼Œä»¥çœŸå¯¦æ¨™ç±¤æ•¸ç‚ºä¸­å¿ƒ
        cluster_candidates = []
        for offset in [-10, -5, 0, 5, 10]:
            n_clusters = n_unique_labels + offset
            if 2 <= n_clusters <= max_clusters:
                cluster_candidates.append(n_clusters)
        
        # å¦‚æœæ²’æœ‰åˆé©çš„å€™é¸ï¼Œä½¿ç”¨ä¿å®ˆçš„é¸æ“‡
        if not cluster_candidates:
            cluster_candidates = [min(max_clusters, max(2, n_unique_labels))]
        
        logger.info(f"Testing hierarchical clustering with {cluster_candidates} clusters (samples: {n_samples}, unique labels: {n_unique_labels})")
        
        for n_clusters in cluster_candidates:
            try:
                hier = AgglomerativeClustering(n_clusters=n_clusters, linkage='average', metric='cosine')
                hier_labels = hier.fit_predict(embeddings)
                hier_ami = adjusted_mutual_info_score(labels, hier_labels)
                clustering_results[f'hierarchical_{n_clusters}'] = hier_ami
            except Exception as e:
                logger.warning(f"Hierarchical clustering with {n_clusters} clusters failed: {e}")
        
        # æ‰¾åˆ°æœ€ä½³çµæœ
        best_method = max(clustering_results.items(), key=lambda x: x[1])
        final_ami = best_method[1]
        
        logger.info(f"Best clustering: {best_method[0]} with AMI {final_ami:.4f}")
        logger.info(f"All results: {clustering_results}")
        
        return final_ami

def main():
    """ä¸»è¦çš„çœŸæ­£å¤šå°ºåº¦è¨“ç·´æµç¨‹"""
    logger.info("=== True Multi-Scale Training for AMI 0.95 Target ===")
    
    # é…ç½®
    model_name = "sentence-transformers/all-mpnet-base-v2"
    PATH2DOWN = os.path.expanduser('~/111111ForDownload/')
    path4ruleGT = PATH2DOWN + 'csv_TLR001_22nm_with_init_cluster_with_gt_class_name.csv'
    model_path = './rule-embedder-0722_trained_MPnet_CachedMN'  # æœ€ä½³ç¾æœ‰æ¨¡å‹
    
    # è¼¸å‡ºè·¯å¾‘
    date_str = datetime.now().strftime('%m%d')
    output_path = f'./rule-embedder-{date_str}_true_multi_scale'
    
    # åˆå§‹åŒ–è¨“ç·´å™¨
    trainer = TrueMultiScaleTrainer(model_name=model_name)
    trainer.load_model()
    
    # æº–å‚™æ•¸æ“š
    train_examples, val_data_triplets, val_sentences, val_labels = trainer.prepare_data(
        path4ruleGT, model_path, enhanced_triplet=True, difficulty_strategy='mixed'
    )
    
    # è¨“ç·´é…ç½® - æ¸›å°‘batch sizeä»¥é¿å…OOM
    training_config = {
        'epochs': 4,
        'batch_size': 4,  # æ¸›å°‘batch sizeå¾8åˆ°4
        'learning_rate': 8e-6,  # è¼ƒä¿å®ˆçš„å­¸ç¿’ç‡
    }
    
    logger.info(f"Training configuration: {training_config}")
    
    # è¨“ç·´çœŸæ­£çš„å¤šå°ºåº¦æ¨¡å‹
    final_ami = trainer.train_true_multi_scale_model(
        train_examples=train_examples,
        val_data=val_data_triplets,
        val_sentences=val_sentences,
        val_labels=val_labels,
        output_path=output_path,
        **training_config
    )
    
    # åŸºç·šæ¯”è¼ƒ
    logger.info("\n=== Baseline Comparison ===")
    baseline_model = SentenceTransformer('./rule-embedder-0722_trained_MPnet_CachedMN')
    baseline_embeddings = baseline_model.encode(val_sentences, show_progress_bar=True)
    baseline_dbscan = DBSCAN(eps=0.095, min_samples=2, metric='cosine')
    baseline_labels = baseline_dbscan.fit_predict(baseline_embeddings)
    
    # æ‡‰ç”¨å€‹åˆ¥å™ªè²ç­–ç•¥åˆ°åŸºç·š
    max_label = max(baseline_labels) if len(baseline_labels) > 0 and max(baseline_labels) >= 0 else -1
    noise_mask = baseline_labels == -1
    n_noise = np.sum(noise_mask)
    if n_noise > 0:
        baseline_labels[noise_mask] = range(max_label + 1, max_label + 1 + n_noise)
    
    baseline_ami = adjusted_mutual_info_score(val_labels, baseline_labels)
    improvement = final_ami - baseline_ami
    
    # çµæœç¸½çµ
    logger.info("\n" + "="*60)
    logger.info("TRUE MULTI-SCALE TRAINING RESULTS")
    logger.info("="*60)
    logger.info(f"Target AMI:           0.9500")
    logger.info(f"Achieved AMI:         {final_ami:.4f}")
    logger.info(f"Baseline AMI:         {baseline_ami:.4f}")
    logger.info(f"Improvement:          {improvement:+.4f}")
    logger.info(f"Gap to target:        {0.95 - final_ami:.4f}")
    
    if final_ami >= 0.95:
        logger.info("ğŸ‰ TARGET ACHIEVED! AMI â‰¥ 0.95")
    elif final_ami >= 0.92:
        logger.info("ğŸ”¥ Excellent progress! Very close to target")
    elif improvement > 0:
        logger.info("âœ… Improvement achieved!")
    else:
        logger.info("âŒ No improvement over baseline")
    
    # ä¿å­˜æœ€çµ‚çµæœ
    final_results = {
        'final_ami': final_ami,
        'baseline_ami': baseline_ami,
        'improvement': improvement,
        'target_achieved': final_ami >= 0.95,
        'training_config': training_config,
        'architecture': 'true_multi_scale_with_orthogonal_constraints'
    }
    
    results_path = os.path.join(output_path, 'final_results.json')
    with open(results_path, 'w') as f:
        json.dump(final_results, f, indent=2)
    
    logger.info(f"\nğŸ“ Model and results saved to: {output_path}")
    logger.info(f"ğŸ“Š Results saved to: {results_path}")
    
    return final_ami

if __name__ == "__main__":
    main()